{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Nikolaos Giannopoulos AM 5199\n",
    "### Team: Trump Tariffed My Datasets"
   ],
   "id": "22d8e7738ea45d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, log_loss"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train = np.load(\"X_train.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "X_test = np.load(\"X_test.npy\")"
   ],
   "id": "1f338fdf356c33e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# XGBoost Hyper-Tuning",
   "id": "68426d029ba2e214"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Best is trial 105 with value: 0.10577266665304749 and {'n_estimators': 491, 'max_depth': 8, 'subsample': 0.8979238963378958, 'colsample_bytree': 0.7931631519474577, 'min_child_weight': 4}\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 300, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.05, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'eval_metric': 'logloss',\n",
    "        'use_label_encoder': False\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params,  device='cuda')\n",
    "    return -cross_val_score(model, X_train, y_train, scoring='neg_log_loss', cv=5).mean()\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=150)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)"
   ],
   "id": "d2d17081352ec043",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Learning rate 0.05 gives the best results, so let it train and then change the learning rate to 0.05\n",
    "xbg = XGBClassifier(**study.best_trial.params, device='cuda')\n",
    "xbg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xbg.predict_proba(X_test)\n",
    "y_pred_xgb = y_pred_xgb[:,1]\n",
    "print(y_pred_xgb)"
   ],
   "id": "c16c0ad8ecdc7674",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LightGBM Hyper-Tuning",
   "id": "57e0f83fd6b41842"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Best is trial 47 with value: 0.10584998479257306 and {'n_estimators': 905, 'learning_rate': 0.12903899598984012, 'max_depth': 3, 'num_leaves': 93, 'min_child_samples': 77, 'subsample': 0.9799477602168288, 'colsample_bytree': 0.9520928492544529}\n",
    "\n",
    "def objective_lgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**params, random_state=42, device = 'gpu')\n",
    "    return -cross_val_score(model, X_train, y_train, scoring='neg_log_loss', cv=5).mean()\n",
    "\n",
    "study_lgb = optuna.create_study(direction='minimize')\n",
    "study_lgb.optimize(objective_lgb, n_trials=100)\n",
    "print(study_lgb.best_params)"
   ],
   "id": "92293d052752959",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CatBoost Hyper-Tuning",
   "id": "e869a20333987b83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Best is trial 47 with value: 0.10584998479257306 and {'learning_rate': 0.07728841406235608, 'depth': 7, 'l2_leaf_reg': 9.09712685218602, 'border_count': 77}\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "    }\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        **params,\n",
    "        verbose=0,\n",
    "        loss_function='Logloss',\n",
    "        random_seed=42,\n",
    "        task_type=\"GPU\"\n",
    "    )\n",
    "    return -cross_val_score(model, X_train, y_train, scoring='neg_log_loss', cv=5).mean()\n",
    "\n",
    "study_cat = optuna.create_study(direction='minimize')\n",
    "study_cat.optimize(objective_cat, n_trials=50)\n",
    "print(study_cat.best_params)"
   ],
   "id": "c34d3108a98a17a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Reduce learning rate to 1/3, increase border to double\n",
    "catboost = CatBoostClassifier(verbose=0,\n",
    "        loss_function='Logloss',\n",
    "        task_type=\"GPU\",learning_rate = 0.025, depth = 7.0, l2_leaf_reg = 9.09712685218602, border_count = 140, iterations = 10000, early_stopping_rounds=2000)\n",
    "catboost.fit(X_train, y_train)\n",
    "y_pred_cat = catboost.predict_proba(X_test)\n",
    "y_pred_cat = y_pred_cat[:,1]\n",
    "print(y_pred_cat)"
   ],
   "id": "8c2b08e57310b04a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Logistic Regression Hyper-Tuning",
   "id": "c894ed5937b751a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = LogisticRegression(solver='liblinear', random_state=42, n_jobs=10)\n",
    "\n",
    "logloss = -cross_val_score(model, X_train, y_train, scoring='neg_log_loss', cv=5).mean()\n",
    "\n",
    "print(\"Log-loss of Logistic Regression is \",logloss)"
   ],
   "id": "7dcf9837bbe69b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Random Forest Hyper-Tuning",
   "id": "b88b32cf69646cc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1)\n",
    "\n",
    "logloss = -cross_val_score(model, X_train, y_train, scoring='neg_log_loss', cv=5).mean()\n",
    "\n",
    "print(\"Log-loss of Random Forest is \",logloss)"
   ],
   "id": "2be8436b213cf780",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Stacking",
   "id": "601a01e2ad73f756"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Model Stacking\n",
    "#CatBoost + XGBoost: Log-loss of Stacked Models is  0.12020644024186593\n",
    "#Log-loss of Stacked Models is  0.1443349556964726\n",
    "best_xgb_model = XGBClassifier(colsample_bytree = 0.9, learning_rate = 0.05, max_depth = 5, min_child_weight = 6, subsample = 0.8864966515073411, n_estimators = 494, device='cuda')\n",
    "best_cat_model = CatBoostClassifier(verbose=0, loss_function='Logloss', task_type=\"GPU\",learning_rate = 0.025, depth = 7.0, l2_leaf_reg = 9.09712685218602, border_count = 140, iterations = 10000, early_stopping_rounds=2000)\n",
    "best_light_model = LGBMClassifier(n_estimators = 905, learning_rate = 0.12903899598984012, max_depth = 3, num_leaves = 93, min_child_samples = 77, subsample = 0.9799477602168288, colsample_bytree = 0.9520928492544529, device = 'gpu')\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', best_xgb_model),  #XGBClassifier(**best_xgb_params)\n",
    "        ('cat', best_cat_model),  #CatBoostClassifier(**best_cat_params)\n",
    "        ('light', best_light_model), #LGBMClassifier(**best_light_params)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5,\n",
    "    stack_method='predict_proba'  #needed for log loss\n",
    ")\n",
    "logloss = -cross_val_score(stacked_model, X_train, y_train, scoring='neg_log_loss', cv=2).mean()\n",
    "stacked_model.fit(X_train, y_train)\n",
    "y_pred_stacked = stacked_model.predict_proba(X_test)\n",
    "y_pred_stacked = y_pred_stacked[:,1]\n",
    "print(\"Log-loss of Stacked Models is \",logloss)\n",
    "print(y_pred_stacked)"
   ],
   "id": "6a8985822d3a9d67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Write predictions to a file\n",
    "predictions = zip(range(len(y_pred_xgb)), y_pred_xgb)\n",
    "with open(\"submissions.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['ID','Label'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ],
   "id": "842ad002443f0c72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_xgb_model = XGBClassifier(colsample_bytree = 0.9, learning_rate = 0.05, max_depth = 5, min_child_weight = 6, subsample = 0.8864966515073411, n_estimators = 494, device='cuda')\n",
    "best_cat_model = CatBoostClassifier(verbose=0, loss_function='Logloss', task_type=\"GPU\",learning_rate = 0.025, depth = 7.0, l2_leaf_reg = 9.09712685218602, border_count = 140, iterations = 10000, early_stopping_rounds=2000)\n",
    "best_light_model = LGBMClassifier(n_estimators = 905, learning_rate = 0.12903899598984012, max_depth = 3, num_leaves = 93, min_child_samples = 77, subsample = 0.9799477602168288, colsample_bytree = 0.9520928492544529, device = 'gpu')\n",
    "\n",
    "logloss_scorer = make_scorer(log_loss, needs_proba=True)\n",
    "scores = cross_val_score(best_cat_model, X_train, y_train, cv=10, scoring='neg_log_loss')\n",
    "print(\"Cross-validated log loss:\", -scores.mean())  #take negative since sklearn returns negative log loss"
   ],
   "id": "b21da2814d90dd85",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
