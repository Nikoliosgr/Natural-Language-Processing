{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Nikolaos Giannopoulos AM 5199\n",
    "### Team: Trump Tariffed My Datasets"
   ],
   "id": "bcb71d82e11d1778"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import losses\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, InputExample"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the data",
   "id": "1296fa7c9402d8df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T17:41:23.841617Z",
     "start_time": "2025-06-11T17:41:23.709565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "node_pairs = pd.read_csv('Data/train_pairs.csv', header=None)\n",
    "labels = pd.read_csv('Data/train_labels.csv', header=None)\n",
    "#Split to use for training and validation\n",
    "train_pairs, val_pairs, train_labels, val_labels = train_test_split(node_pairs, labels, train_size=0.8, test_size=0.2, random_state=42)\n",
    "\n",
    "#Train pairs for SPECTER fine-tuning, val pairs for validation of each model\n",
    "train_pairs = train_pairs.reset_index(drop=True)\n",
    "val_pairs = val_pairs.reset_index(drop=True)\n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "val_labels = val_labels.reset_index(drop=True)"
   ],
   "id": "40071d39764b669f",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T17:41:24.888095Z",
     "start_time": "2025-06-11T17:41:24.476388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the abstract of each paper\n",
    "abstracts = dict()\n",
    "with open('abstracts.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        node, abstract = line.split('|--|')\n",
    "        abstracts[int(node)] = abstract"
   ],
   "id": "674e479023e0bece",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 59,
   "source": "length_total = train_pairs.shape[0]",
   "id": "c4566ab8879cd7b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare the abstracts for SPECTER fine-tuning",
   "id": "429169503e39fa10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:58:38.012547Z",
     "start_time": "2025-06-02T15:57:12.481093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "abstracts_all = []\n",
    "\n",
    "length_pairs = train_pairs.shape[0]\n",
    "\n",
    "for i in tqdm(range(length_pairs)):\n",
    "    abstract_1 = int(train_pairs.iloc[[i]][0].item())\n",
    "    abstract_2 = int(train_pairs.iloc[[i]][1].item())\n",
    "    linked = train_labels.iloc[[i]][0].item()\n",
    "    if linked==0:\n",
    "        abstracts_all.append(InputExample(texts=[abstracts[abstract_1], abstracts[abstract_2]], label=0.0))\n",
    "    else:\n",
    "        abstracts_all.append(InputExample(texts=[abstracts[abstract_1], abstracts[abstract_2]], label=1.0))"
   ],
   "id": "8da03c1d542b376",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327585/327585 [01:25<00:00, 3830.37it/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training area for the fine-tuned SPECTER ",
   "id": "cdedd0228388476"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T15:31:34.326852Z",
     "start_time": "2025-06-15T15:31:34.305298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "id": "27888f973eea19c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T18:56:23.608244Z",
     "start_time": "2025-06-02T15:58:57.073198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Make sure GPU memory is clean\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Load pretrained SPECTER\n",
    "model = SentenceTransformer('sentence-transformers/allenai-specter', device=device)\n",
    "\n",
    "#Create DataLoader for all training pairs\n",
    "train_dataloader = DataLoader(abstracts_all, shuffle=True, batch_size=16)  #Batch size 16-32\n",
    "\n",
    "#Define cosine similarity loss\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#Train the model\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,                  #Try 3 because it takes a lot of time and huge load\n",
    "    warmup_steps=100,\n",
    "    show_progress_bar=True,\n",
    "    output_path='models/specter_finetuned_full_test',\n",
    "    use_amp=True\n",
    ")\n",
    "model.save('models/specter_finetuned_full_test')\n"
   ],
   "id": "c6b1d3a65a7376c3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicks\\PycharmProjects\\LinkPrediction\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02fa22b0b9d943388c6ad5e28914a272"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61425' max='61425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61425/61425 2:57:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.093300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.091300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.087400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.082300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.079700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.076600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.073800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.071700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.062600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.060900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.061600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.061900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.059300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.061700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.061900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.053300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.049900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model training area",
   "id": "b0562ff7f278c22f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T16:55:10.734666Z",
     "start_time": "2025-06-11T16:54:40.148155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"specter_finetuned_full_test\"  # Choose: \"bert\", \"scibert\", \"specter\" and 'specter_finetuned_test_full\"\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----- INPUT -----\n",
    "nodes = list(abstracts.keys())\n",
    "texts = [abstracts[int(i)] for i in nodes]\n",
    "txt2feat = dict()\n",
    "article_textual_embeddings = []\n",
    "\n",
    "# -------------- BATCH UTILS --------------\n",
    "def batched(iterable, batch_size):\n",
    "    for i in range(0, len(iterable), batch_size):\n",
    "        yield iterable[i:i + batch_size]\n",
    "\n",
    "# -------------- MODEL LOAD AND BATCH EMBEDDING --------------\n",
    "if model_name == \"bert\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = AutoModel.from_pretrained('bert-base-uncased', torch_dtype=torch.float16).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for batch_nodes in tqdm(list(batched(nodes, batch_size))):\n",
    "        texts_batch = [abstracts[int(i)] for i in batch_nodes]\n",
    "        tokens = tokenizer(texts_batch, return_tensors='pt', padding=True, truncation=True, max_length=250).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens).pooler_output.cpu().numpy()\n",
    "        for i, node in enumerate(batch_nodes):\n",
    "            txt2feat[int(node)] = outputs[i]\n",
    "\n",
    "elif model_name == \"scibert\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "    model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased').to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for batch_nodes in tqdm(list(batched(nodes, batch_size))):\n",
    "        texts_batch = [abstracts[int(i)] for i in batch_nodes]\n",
    "        tokens = tokenizer(texts_batch, return_tensors='pt', padding=True, truncation=True, max_length=250).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens).pooler_output.cpu().numpy()\n",
    "        for i, node in enumerate(batch_nodes):\n",
    "            txt2feat[int(node)] = outputs[i]\n",
    "\n",
    "elif model_name == \"specter\":\n",
    "    model = SentenceTransformer('sentence-transformers/allenai-specter').to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for batch_nodes in tqdm(list(batched(nodes, batch_size))):\n",
    "        texts_batch = [abstracts[int(i)] for i in batch_nodes]\n",
    "        embeddings = model.encode(texts_batch, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=False)\n",
    "        for i, node in enumerate(batch_nodes):\n",
    "            txt2feat[int(node)] = embeddings[i]\n",
    "\n",
    "elif model_name == \"specter_finetuned_full_test\":\n",
    "    model = SentenceTransformer('models/specter_finetuned_full_test').to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for batch_nodes in tqdm(list(batched(nodes, batch_size))):\n",
    "        texts_batch = [abstracts[int(i)] for i in batch_nodes]\n",
    "        embeddings = model.encode(texts_batch, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=False)\n",
    "        for i, node in enumerate(batch_nodes):\n",
    "            txt2feat[int(node)] = embeddings[i]\n",
    "            \n",
    "# --------------OOUTPUT AS ARRAY --------------\n",
    "for i in nodes:\n",
    "    article_textual_embeddings.append(txt2feat[int(i)])"
   ],
   "id": "869e9efd7fc7c064",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 116/4329 [00:29<18:07,  3.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 59\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_nodes \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mlist\u001B[39m(batched(nodes, batch_size))):\n\u001B[0;32m     58\u001B[0m     texts_batch \u001B[38;5;241m=\u001B[39m [abstracts[\u001B[38;5;28mint\u001B[39m(i)] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m batch_nodes]\n\u001B[1;32m---> 59\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_to_numpy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, node \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(batch_nodes):\n\u001B[0;32m     61\u001B[0m         txt2feat[\u001B[38;5;28mint\u001B[39m(node)] \u001B[38;5;241m=\u001B[39m embeddings[i]\n",
      "File \u001B[1;32m~\\PycharmProjects\\LinkPrediction\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:720\u001B[0m, in \u001B[0;36mSentenceTransformer.encode\u001B[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001B[0m\n\u001B[0;32m    718\u001B[0m             \u001B[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001B[39;00m\n\u001B[0;32m    719\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m convert_to_numpy:\n\u001B[1;32m--> 720\u001B[0m                 embeddings \u001B[38;5;241m=\u001B[39m \u001B[43membeddings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    722\u001B[0m         all_embeddings\u001B[38;5;241m.\u001B[39mextend(embeddings)\n\u001B[0;32m    724\u001B[0m all_embeddings \u001B[38;5;241m=\u001B[39m [all_embeddings[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39margsort(length_sorted_idx)]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T19:57:46.746025Z",
     "start_time": "2025-06-02T19:57:41.505453Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(article_textual_embeddings, f'Data/article_textual_embeddings_specter_test.pt')",
   "id": "45dd866120f5712f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test models",
   "id": "aa5fd03c13f0885"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T17:41:56.275641Z",
     "start_time": "2025-06-11T17:41:28.185540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Create the positive and negative sambles\n",
    "pos_samples = []\n",
    "neg_samples = []\n",
    "length_total = val_pairs.shape[0]\n",
    "\n",
    "for i in tqdm(range(length_total)):\n",
    "    linked = val_labels.iloc[[i]][0].item()\n",
    "    if linked==1:\n",
    "        pos_samples.append([int(val_pairs.iloc[[i]][0].item()),int(val_pairs.iloc[[i]][1].item())])\n",
    "    else:\n",
    "        neg_samples.append([int(val_pairs.iloc[[i]][0].item()),int(val_pairs.iloc[[i]][1].item())])"
   ],
   "id": "f27aa55d24cd12a4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109196/109196 [00:28<00:00, 3889.74it/s]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T18:15:48.476789Z",
     "start_time": "2025-06-11T18:15:44.951044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Load embeddings\n",
    "full_array = torch.load(\"Data/article_textual_embeddings_specter_test.pt\", weights_only=False)\n",
    "simspos = []\n",
    "simsneg = []\n",
    "\n",
    "#Compute similarities for the first 500 negative pairs\n",
    "for neg_sample in tqdm(neg_samples[:500]):\n",
    "    #Get the embeddings of the two articles in the pair\n",
    "    node_paper_embedding_1 = np.expand_dims(full_array[neg_sample[0]], axis=0)\n",
    "    node_paper_embedding_2 = np.expand_dims(full_array[neg_sample[1]], axis=0)\n",
    "\n",
    "    #Compute cosine similarity\n",
    "    paper_sim = cosine_similarity(node_paper_embedding_1, node_paper_embedding_2)[0]\n",
    "    simsneg.append(paper_sim)\n",
    "\n",
    "#Print average similarity\n",
    "print(\"Average cosine similarity on negative:\", sum(simsneg) / len(simsneg))\n",
    "\n",
    "for pos_sample in tqdm(pos_samples[:500]):\n",
    "    \n",
    "    node_paper_embedding_1 = np.expand_dims(full_array[pos_sample[0]], axis=0)\n",
    "    node_paper_embedding_2 = np.expand_dims(full_array[pos_sample[1]], axis=0)\n",
    "    simspos.append(cosine_similarity(node_paper_embedding_1, node_paper_embedding_2)[0])\n",
    "\n",
    "print(\"Average cosine similarity on positive:\", sum(simspos) / len(simspos))"
   ],
   "id": "6b93a09934047f9f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 3897.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine similarity on negative: [0.14071403]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 3395.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine similarity on positive: [0.82576215]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
